{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Program to perform Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "import nltk.tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#creating functions :\n",
    "\n",
    "\n",
    "#loading Positive words\n",
    "with open(\"PositiveWords.txt\",\"r\",encoding=\"utf-8\") as pfile:\n",
    "    positive_words= pfile.read().lower()\n",
    "positive_List = positive_words.split()\n",
    "\n",
    "#loading Negative words\n",
    "with open(\"NegativeWords.txt\",\"r\",encoding=\"utf-8\") as nfile:\n",
    "    negative_word= nfile.read().lower()\n",
    "negative_List = negative_word.split()\n",
    "\n",
    "#will match the words and will give us the scores\n",
    "\n",
    "def positive_score(text):\n",
    "    pos = set(positive_List)&set(text)\n",
    "    return len(pos)\n",
    "\n",
    "\n",
    "def negative_score(text):\n",
    "    nega = set(negative_List)&set(text)\n",
    "    return len(nega)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_sentence_length(text):\n",
    "    sentence = text.split('.')\n",
    "    words = word_tokenize(text)\n",
    "    totalWordCount = len(words)\n",
    "    totalSentences = len(sentence)\n",
    "   \n",
    "    return round(totalWordCount/totalSentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_word_percentage(text):\n",
    "    words = word_tokenize(text)\n",
    "    complexWord = 0\n",
    "    for word in words:\n",
    "        vowels=0\n",
    "        if word.endswith(('es','ed')):\n",
    "            pass\n",
    "        else:\n",
    "            for w in word:\n",
    "                if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):\n",
    "                    vowels += 1\n",
    "            if(vowels > 2):\n",
    "                complexWord += 1\n",
    "    if len(words) != 0:\n",
    "        complex_word_percentage = (complexWord/len(words))*100\n",
    "        return round(complex_word_percentage,2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fogindex():\n",
    "    fogIndex = 0.4 * (average_sentence_length(para) + complex_word_percentage(gem))\n",
    "    return fogIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def average_words_sentence(para):\n",
    "    sentence=para.split('.')\n",
    "    word=clean_words\n",
    "    return (len(word)/len(sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "#considering words after removing stop words\n",
    "\n",
    "def complex_word():\n",
    "    words = clean_words\n",
    "    complexWord = 0\n",
    "    for word in words:\n",
    "        vowels=0\n",
    "        if word.endswith(('es','ed')):\n",
    "            pass\n",
    "        else:\n",
    "            for w in word:\n",
    "                if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):\n",
    "                    vowels += 1\n",
    "            if(vowels > 2):\n",
    "                complexWord += 1\n",
    "    if len(words) != 0:\n",
    "        \n",
    "        return round(complexWord)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [],
   "source": [
    "#considering words after removing stop words\n",
    "def word_count(g):\n",
    "    return len(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "#syllable count per word\n",
    "\n",
    "def syllable_count(word):\n",
    "    lord = str(word).lower()\n",
    "    count = 0\n",
    "    vowels = \"aeiouy\"\n",
    "    if lord[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(lord)):\n",
    "        if lord[index] in vowels and lord[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if lord.endswith(\"e\"):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count/len(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Personal_Pronouns(text):\n",
    "    a = re.findall(\"I\",text)\n",
    "    b = re.findall(\"me\",text)\n",
    "    c = re.findall(\"we\",text)\n",
    "    d = re.findall(\"us\",text)\n",
    "    e = re.findall(\"ours\",text)\n",
    "    f=a+b+c+d+e\n",
    "    return len(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [],
   "source": [
    "#considering words after removing stop words\n",
    "def avg_word_length(word):\n",
    "    x=[len(word) for word in word]\n",
    "    y=sum(x)/len(word)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main()\n",
    "\n",
    "df= pd.read_csv('input.csv')\n",
    "x = df['URL_ID'].tolist()\n",
    "y = df['URL'].tolist()\n",
    "s1=[] ;s2=[];s3=[];s4=[];s5=[];s6=[];s7=[];s8=[];s9=[];s10=[];s11=[];s12=[];s13=[]\n",
    "\n",
    "\n",
    "for q ,z in zip(y,x):\n",
    "    urlid = q\n",
    "    df= pd.read_csv('input.csv')\n",
    "    with open (f\"{z}.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "        para=f.read()\n",
    "    words = word_tokenize(para)\n",
    "\n",
    "    alphabetic_only= [word for word in words if word.isalpha()]\n",
    "    lower_case=[word.lower() for word in alphabetic_only]\n",
    "    stop_words= set(stopwords.words('english'))\n",
    "    with open (\"StopWords_GenericLong.txt\",\"r\") as f:\n",
    "        stop_words2 = f.read()\n",
    "    clean_words= [word for word in lower_case if word not in stop_words and stop_words2]\n",
    "    \n",
    "    \n",
    "    ps= positive_score(clean_words)\n",
    "    ns= negative_score(clean_words)\n",
    "    nw= len(clean_words) #number  of words after cleaning\n",
    "\n",
    "    #Polarity_Score = (positive_score(clean_words) - negative_score(clean_words))/ ((positive_score(clean_words) + negative_score(clean_words)) + 0.000001)\n",
    "   \n",
    "    def Polarity_Score():\n",
    "        Polarity_Score = (ps - ns)/ ((ps + ns) + 0.000001)\n",
    "        return Polarity_Score\n",
    "\n",
    "    #Subjectivity_Score = (positive_score(clean_words) + negative_score(clean_words))/ (len(clean_words) + 0.000001)\n",
    "    \n",
    "    def Subjectivity_Score():\n",
    "        Subjectivity_Score = (ps+ ns)/ (nw + 0.000001)\n",
    "        return Subjectivity_Score\n",
    "\n",
    "\n",
    "    a1=positive_score(clean_words)\n",
    "    a2=negative_score(clean_words)\n",
    "    a3=Polarity_Score()\n",
    "    a4=Subjectivity_Score()\n",
    "    a5=round(average_sentence_length(para))   #using round function to get clean data \n",
    "    a6= complex_word_percentage(para)\n",
    "    a7=fogindex()\n",
    "    a8=round(average_words_sentence(para))   #using round function to get clean data \n",
    "    a9=complex_word()\n",
    "    a10=word_count(clean_words)\n",
    "    a11=round(syllable_count(clean_words),2)   #using round function to get clean data \n",
    "    a12=Personal_Pronouns(para)\n",
    "    a13=(avg_word_length(clean_words))\n",
    "    \n",
    "    #adding data to the lists\n",
    "    \n",
    "    s1.append(a1)\n",
    "    s2.append(a2)\n",
    "    s3.append(a3)\n",
    "    s4.append(a4)\n",
    "    s5.append(a5)\n",
    "    s6.append(a6)\n",
    "    s7.append(a7)\n",
    "    s8.append(a8)\n",
    "    s9.append(a9)\n",
    "    s10.append(a10)\n",
    "    s11.append(a11)\n",
    "    s12.append(a12)\n",
    "    s13.append(a13)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "                    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframes\n",
    "\n",
    "S=[x,y,s1,s2,s3,s4,s5,s6,s7,s8,s9,s10,s11,s12,s13]\n",
    "columns=['URL_ID','URL','POSITIVE_SCORE','NEGATIVE SCORE','POLARITY SCORE','SUBJECTIVITY SCORE','AVG SENTENCE LENGTH','PERCENTAGE OF COMPLEX WORDS','FOG INDEX','AVG NUMBER OF WORDS PER SENTENCE','COMPLEX WORD COUNT','WORD COUNT','SYLLABLE PER WORD','PERSONAL PRONOUNS','AVG WORD LENGTH']\n",
    "df=pd.DataFrame(S,columns)\n",
    "result=df.transpose()\n",
    "\n",
    "#creating CSV file\n",
    "result.to_excel('result.xlsx',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
